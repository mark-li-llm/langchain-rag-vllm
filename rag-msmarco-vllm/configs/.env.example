# Environment configuration for RAG MS MARCO template
# Copy this file to .env and customize the values

# ============================================================================
# DATA AND INDEX CONFIGURATION
# ============================================================================

# Hugging Face dataset configuration
HF_DATASET_NAME=microsoft/ms_marco
HF_DATASET_CONFIG=v2.1
HF_SPLIT_CORPUS=train
HF_SPLIT_QUERIES=validation

# Corpus sampling (set to limit dataset size for faster development)
# Set to null or comment out to use full dataset
CORPUS_SAMPLE_SIZE=200000

# ============================================================================
# EMBEDDING MODEL CONFIGURATION  
# ============================================================================

# Embedding model from Hugging Face
# Recommended: BAAI/bge-small-en-v1.5 (384 dims, good performance/speed balance)
# Alternatives: BAAI/bge-base-en-v1.5 (768 dims), sentence-transformers/all-MiniLM-L6-v2
EMBED_MODEL_NAME=BAAI/bge-small-en-v1.5

# ============================================================================
# VECTOR DATABASE CONFIGURATION
# ============================================================================

# Qdrant configuration
INDEX_BACKEND=qdrant
QDRANT_URL=http://localhost:6333
# QDRANT_API_KEY=your_api_key_if_auth_enabled
COLLECTION_NAME=msmarco_chunks_v21

# ============================================================================
# LARGE LANGUAGE MODEL CONFIGURATION
# ============================================================================

# OpenAI-compatible API configuration for vLLM
# This should point to your Nginx load balancer endpoint
OPENAI_API_BASE=http://localhost:8080/llm/v1

# API key (vLLM can be configured to require authentication)
# Set to any value if vLLM doesn't require authentication
OPENAI_API_KEY=your-api-key-here

# Model name as served by vLLM
# This should match the --served-model-name parameter in vLLM
LLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct

# Generation parameters
MAX_OUTPUT_TOKENS=300
TEMPERATURE=0.2

# ============================================================================
# API AND SECURITY CONFIGURATION
# ============================================================================

# Bearer token for API authentication
# Generate a secure random token for production
AUTH_BEARER_TOKEN=your-secure-bearer-token-here

# ============================================================================
# RETRIEVAL CONFIGURATION
# ============================================================================

# Retrieval parameters
TOPK_PRE=50          # Number of candidates from dense retrieval
TOPK_FINAL=5         # Final number of documents to return
USE_BM25=true        # Enable BM25 sparse retrieval in ensemble
USE_RERANKER=false   # Enable reranking (not implemented in base template)

# Context management
MAX_CONTEXT_TOKENS=3000

# ============================================================================
# TEXT PROCESSING CONFIGURATION
# ============================================================================

# Chunking parameters
CHUNK_SIZE_CHARS=1800
CHUNK_OVERLAP_CHARS=200

# ============================================================================
# OPERATIONAL CONFIGURATION
# ============================================================================

# Logging
LOG_LEVEL=info

# API server configuration
UVICORN_WORKERS=2

# Observability (optional)
PROMETHEUS_PORT=9100
# OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger:14268/api/traces

# ============================================================================
# DOCKER COMPOSE CONFIGURATION
# ============================================================================

# Grafana configuration (for monitoring profile)
GRAFANA_PASSWORD=admin
GRAFANA_DOMAIN=localhost

# ============================================================================
# DEVELOPMENT AND TESTING
# ============================================================================

# Development flags
DEBUG=false

# Testing configuration
TEST_QUERIES_FILE=configs/test_queries.txt

# ============================================================================
# EXTERNAL VLLM SERVERS
# ============================================================================

# vLLM Server A and B configuration
# These are referenced in nginx.conf - update the IPs/hostnames there
# 
# Example vLLM server A:
# VLLM_A_HOST=192.168.1.100
# VLLM_A_PORT=8000
#
# Example vLLM server B: 
# VLLM_B_HOST=192.168.1.101
# VLLM_B_PORT=8000

# ============================================================================
# PRODUCTION CONSIDERATIONS
# ============================================================================

# For production deployment:
# 1. Generate secure AUTH_BEARER_TOKEN (use: openssl rand -hex 32)
# 2. Set appropriate CORS origins in the API
# 3. Configure proper TLS termination
# 4. Set up log aggregation and monitoring
# 5. Configure resource limits in Docker Compose
# 6. Set up backup for Qdrant data volume
# 7. Monitor vLLM server health and failover
# 8. Consider using external secrets management

# ============================================================================
# QUICK START COMMANDS
# ============================================================================

# 1. Copy this file: cp configs/.env.example .env
# 2. Edit the values above, especially:
#    - AUTH_BEARER_TOKEN
#    - OPENAI_API_KEY  
#    - OPENAI_API_BASE (if vLLM servers are not on localhost)
# 3. Build index: make build-index
# 4. Start services: make serve
# 5. Test: curl -H "Authorization: Bearer YOUR_TOKEN" http://localhost:8080/v1/health